{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling climat\n",
    "\n",
    "Ce notebook a pour objectif de regarde les sujets traiter par chacunes des communautés principales et\n",
    "de les comparer à ce qui se fait dans les médias pro et climato_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/home/tanguyruault/Documents/Tanguy/hue\n"
     ]
    }
   ],
   "source": [
    "%cd /\n",
    "%cd /home/tanguyruault/Documents/Tanguy/hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ouverture du fichier\n",
    "with open('both_com_texte.txt', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pour faciliter le traitement, on convertit le fichier en une liste de tokens que l'on découpe ensuite en fonction des lignes\n",
    "file = []\n",
    "for raw in lines :\n",
    "    word = raw.split(\" \")\n",
    "    file.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tanguyruault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#from spacy.lang.en import LOOKUP\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import argparse\n",
    "import emoji\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "#on enlève dans un premier temps:\n",
    "#mentions\n",
    "#liens https\n",
    "#ponctuations\n",
    "#stopwords\n",
    "\n",
    "#Methode perso\n",
    "#d = re.sub(r'[^\\w\\s]', \"\", corpus)\n",
    "#d =  re.sub(r'@([A-Za-z0-9]*):',\"\", d)\n",
    "#d = re.sub(r'tx|\\\\n', \"\", d)\n",
    "#mots clés en anglais\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punct = set(string.punctuation)\n",
    "clean = []\n",
    "\n",
    "for element in file :# Convert unicode emoji to shortcode emoji\n",
    "     element = emoji.demojize(str(element))\n",
    "         \n",
    "        # Remove all mentions (@) in the onon\n",
    "     element = re.sub(r'@\\S+ *', '', element)\n",
    "          # Remove all hashtags (#) in the row\n",
    "     element = re.sub(r'#\\S+ *', '', element)\n",
    "            #remove all rt in the row\n",
    "     element = re.sub(r'rt', '',element)\n",
    "    \n",
    "   ####remove isolated numbers\n",
    "     element = re.sub(r'\\W+\\d+', '', element)\n",
    "     element = re.sub(r'[\\\\\\;\\']', '', element)\n",
    "     #remove spaces pour prendre en compte les alphanum\n",
    "     element = re.sub(r'\\s', '', element)\n",
    "     element = element.split(\",\")\n",
    "         \n",
    "        ###conversion en liste de element et de word en str\n",
    "\n",
    "     element = [word.lower() for word in element if re.sub('\\W', '', word)]\n",
    "     #enlever les url\n",
    "     #element = [word for word in element if not urlparse(word).scheme]\n",
    "     #element = [str(word) for word in element]\n",
    "     #element =   [char for char in element if char not in punct]\n",
    "     #element = [' '.join(word) for word in element] \n",
    "     element = [word for word in element if word.isalpha()]\n",
    "     element= [word for word in element if word not in stopwords]\n",
    "     clean.append(element)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sea', 'ice', 'still', 'formed', 'siberia', 'scientists', 'worried']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46686"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in clean:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "len(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in clean\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "# Begin building the topic model\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Size of dictionary: 30294\n"
     ]
    }
   ],
   "source": [
    "# Print information on the document corpus\n",
    "print(\"[INFO] Size of dictionary: {}\".format(len(dictionary)))\n",
    "# Convert the dictionary into a document term matrix\n",
    "matrix = [dictionary.doc2bow(sent) for sent in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essai avec un seul dataset de topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-89fbb5142ac6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# basic plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'basic plot'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Initialize the LDA model from gensim\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train the model on the document term matrix\n",
    "model = lda(matrix, num_topics=10, id2word=dictionary, passes=10,\n",
    "            iterations=10)\n",
    "    # Initialize the coherence evaluation model from gensim\n",
    "cm = gensim.models.coherencemodel.CoherenceModel(model=model,\n",
    "                                                 texts=texts,\n",
    "                                                 coherence='c_v',\n",
    "                                                 topn=10\n",
    "                                                 )\n",
    "    # Get the coherence of each topic and round the values : pour chaque itération, nombre de cohérences correspond au nombre de topics. \n",
    "values = cm.get_coherence_per_topic()\n",
    "coherence = [round(score, 3) for score in values] \n",
    "    # Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "    \n",
    "    ##on transforme la cohérence en array\n",
    "coherence = np.array(coherence)\n",
    "spread = coherence.std()\n",
    "center = coherence.mean()\n",
    "flier_high = coherence[len(coherence)-1]\n",
    "\n",
    "flier_low = coherence[0]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# basic plot\n",
    "axs[1,1].boxplot(data)\n",
    "axs[1,1].set_title('basic plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08642829397830319\n",
      "0.3215\n",
      "0.255\n",
      "0.442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08642829, 0.3215    , 0.255     , 0.442     ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spread)\n",
    "print(center)\n",
    "print(flier_high)\n",
    "print(flier_low)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec un nombre fixe de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9\n",
    "# Initialize the LDA model from gensim\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train the model on the document term matrix\n",
    "model = lda(matrix, num_topics=size, id2word=dictionary, passes=size,\n",
    "            iterations=size)\n",
    "    # Initialize the coherence evaluation model from gensim\n",
    "cm = gensim.models.coherencemodel.CoherenceModel(model=model,\n",
    "                                                 texts=texts,\n",
    "                                                 coherence='c_v',\n",
    "                                                 topn=size\n",
    "                                                 )\n",
    "    # Get the coherence of each topic and round the values : pour chaque itération, nombre de cohérences correspond au nombre de topics. \n",
    "values = cm.get_coherence_per_topic()\n",
    "coherence = [round(score, 3) for score in values] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notched plot\n",
    "axs[0, 1].boxplot(data, 1)\n",
    "axs[0, 1].set_title('notched plot')\n",
    "\n",
    "# change outlier point symbols\n",
    "axs[0, 2].boxplot(data, 0, 'gD')\n",
    "axs[0, 2].set_title('change outlier\\npoint symbols')\n",
    "\n",
    "# don't show outlier points\n",
    "axs[1, 0].boxplot(data, 0, '')\n",
    "axs[1, 0].set_title(\"don't show\\noutlier points\")\n",
    "\n",
    "# horizontal boxes\n",
    "axs[1, 1].boxplot(data, 0, 'rs', 0)\n",
    "axs[1, 1].set_title('horizontal boxes')\n",
    "\n",
    "# change whisker length\n",
    "axs[1, 2].boxplot(data, 0, 'rs', 0, 0.75)\n",
    "axs[1, 2].set_title('change whisker length')\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3)\n",
    "\n",
    "# fake up some more data\n",
    "spread = np.random.rand(50) * 100\n",
    "center = np.ones(25) * 40\n",
    "flier_high = np.random.rand(10) * 100 + 100\n",
    "flier_low = np.random.rand(10) * -100\n",
    "d2 = np.concatenate((spread, center, flier_high, flier_low))\n",
    "# Making a 2-D array only works if all the columns are the\n",
    "# same length.  If they are not, then use a list instead.\n",
    "# This is actually more efficient because boxplot converts\n",
    "# a 2-D array into a list of vectors internally anyway.\n",
    "data = [data, d2, d2[::2]]\n",
    "\n",
    "# Multiple box plots on one Axes\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ice</td>\n",
       "      <td>global</td>\n",
       "      <td>carbon</td>\n",
       "      <td>fossil</td>\n",
       "      <td>climate</td>\n",
       "      <td>new</td>\n",
       "      <td>via</td>\n",
       "      <td>climate</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>years</td>\n",
       "      <td>warming</td>\n",
       "      <td>emissions</td>\n",
       "      <td>fuel</td>\n",
       "      <td>change</td>\n",
       "      <td>energy</td>\n",
       "      <td>great</td>\n",
       "      <td>change</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arctic</td>\n",
       "      <td>due</td>\n",
       "      <td>gas</td>\n",
       "      <td>every</td>\n",
       "      <td>action</td>\n",
       "      <td>time</td>\n",
       "      <td>look</td>\n",
       "      <td>crisis</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sea</td>\n",
       "      <td>caused</td>\n",
       "      <td>extreme</td>\n",
       "      <td>get</td>\n",
       "      <td>policy</td>\n",
       "      <td>see</td>\n",
       "      <td>fire</td>\n",
       "      <td>need</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>last</td>\n",
       "      <td>level</td>\n",
       "      <td>weather</td>\n",
       "      <td>fuels</td>\n",
       "      <td>government</td>\n",
       "      <td>us</td>\n",
       "      <td>shows</td>\n",
       "      <td>world</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>greenhouse</td>\n",
       "      <td>nothing</td>\n",
       "      <td>oil</td>\n",
       "      <td>life</td>\n",
       "      <td>un</td>\n",
       "      <td>could</td>\n",
       "      <td>activists</td>\n",
       "      <td>make</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>human</td>\n",
       "      <td>cooling</td>\n",
       "      <td>arctic</td>\n",
       "      <td>less</td>\n",
       "      <td>trump</td>\n",
       "      <td>real</td>\n",
       "      <td>talk</td>\n",
       "      <td>fight</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>since</td>\n",
       "      <td>heat</td>\n",
       "      <td>temperature</td>\n",
       "      <td>humans</td>\n",
       "      <td>pa</td>\n",
       "      <td>repo</td>\n",
       "      <td>made</td>\n",
       "      <td>must</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>past</td>\n",
       "      <td>happening</td>\n",
       "      <td>fires</td>\n",
       "      <td>evidence</td>\n",
       "      <td>new</td>\n",
       "      <td>polar</td>\n",
       "      <td>kids</td>\n",
       "      <td>future</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.471</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1          2            3         4           5       6  \\\n",
       "0         ice     global       carbon    fossil     climate     new   \n",
       "1       years    warming    emissions      fuel      change  energy   \n",
       "2      arctic        due          gas     every      action    time   \n",
       "3         sea     caused      extreme       get      policy     see   \n",
       "4        last      level      weather     fuels  government      us   \n",
       "5  greenhouse    nothing          oil      life          un   could   \n",
       "6       human    cooling       arctic      less       trump    real   \n",
       "7       since       heat  temperature    humans          pa    repo   \n",
       "8        past  happening        fires  evidence         new   polar   \n",
       "9       0.471      0.426          0.3     0.368       0.357   0.274   \n",
       "\n",
       "           7        8        9  \n",
       "0        via  climate  science  \n",
       "1      great   change   people  \n",
       "2       look   crisis      say  \n",
       "3       fire     need     know  \n",
       "4      shows    world    think  \n",
       "5  activists     make     like  \n",
       "6       talk    fight    would  \n",
       "7       made     must   really  \n",
       "8       kids   future  believe  \n",
       "9      0.256    0.432    0.421  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print topics\n",
    "topics = model.show_topics(num_topics=size, num_words=size, formatted=True)\n",
    "# Add a placeholder list to hold the final output\n",
    "result = []\n",
    "\n",
    "# Loop over the topics\n",
    "for t in topics:\n",
    "\n",
    "    # Assign the word string into their own variable\n",
    "    words = t[1]\n",
    "\n",
    "    # Split the words\n",
    "    words = words.split(' + ')\n",
    "\n",
    "    # Extract the words from the predictions\n",
    "    words = [w.split('*')[1].strip('\"') for w in words]\n",
    "\n",
    "    # Append the row to the final result\n",
    "    result.append(words)\n",
    "\n",
    "# Convert the result into a NumPy array and transpose.\n",
    "result = np.vstack(result).transpose()\n",
    "\n",
    "# Append the coherence scores (Cv) to the matrix\n",
    "result = np.vstack([result, coherence])\n",
    "\n",
    "# Convert the result into a DataFrame\n",
    "result = pd.DataFrame(result, columns=range(1, len(result)))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " both_com_texte.csv                             'multilayer_test_th=20.csv'\n",
      " both_com_texte.txt                             'multilayer_th=2.csv'\n",
      "'climateEdge_graph_01102019_30062020_th=2.csv'  'multlayer_test_th=10.csv'\n",
      " \u001b[0m\u001b[01;35mcoherence0-10.png\u001b[0m                              'tableau_tm_tnb=5.tex'\n",
      " multilayer_test1.csv\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_latex(\"tableau_tm_tnb=9.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-a5c848965d1e>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-a5c848965d1e>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    In [7]:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print out a LaTeX table\n",
    "print(result.to_latex(na_rep='--', index=False))\n",
    "In [ ]:\n",
    "\n",
    "In [ ]:\n",
    "\n",
    "In [39]:\n",
    "sents = []\n",
    "if len(row) > 0:\n",
    "            row = row.split()\n",
    "            sents.append(row)\n",
    "Out[39]:\n",
    "''\n",
    "In [34]:\n",
    "# calcul des bigrams\n",
    "bigrams_list = []\n",
    "for i in range(0 , len(text) - 1):\n",
    "  bigram = text[i] + \" \" + text[i+1]\n",
    "  bigrams_list.append(bigram)\n",
    "\n",
    "print(bigrams_list[1])\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-34-44a097b82998> in <module>\n",
    "      1 # calcul des bigrams\n",
    "      2 bigrams_list = []\n",
    "----> 3 for i in range(0 , len(text) - 1):\n",
    "      4   bigram = text[i] + \" \" + text[i+1]\n",
    "      5   bigrams_list.append(bigram)\n",
    "\n",
    "NameError: name 'text' is not defined\n",
    "In [95]:\n",
    "#calcul des trigrams\n",
    "\n",
    "trigrams_list = []\n",
    "for i in range(0 , len(text) - 2):\n",
    "  trigram = text[i] + \" \" + text[i+1] + \" \" + text[i+2]\n",
    "  trigrams_list.append(trigram)\n",
    "\n",
    "print(trigrams_list[2])\n",
    "change predictive garbage\n",
    "In [1]:\n",
    "# Count word frequencies\n",
    "frequency = defaultdict(int)\n",
    "for token in text:\n",
    "        frequency[token] += 1    \n",
    "        \n",
    "freq = pd.DataFrame.from_dict(frequency, orient = \"index\").sort_values(by=[0], ascending = False)\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-1-cdf0c2007d51> in <module>\n",
    "      1 # Count word frequencies\n",
    "----> 2 frequency = defaultdict(int)\n",
    "      3 for token in text:\n",
    "      4         frequency[token] += 1\n",
    "      5 \n",
    "\n",
    "NameError: name 'defaultdict' is not defined\n",
    "In [102]:\n",
    "# Count bigrams frequencies\n",
    "bi_frequency = defaultdict(int)\n",
    "for token in bigrams_list:\n",
    "        bi_frequency[token] += 1\n",
    "        \n",
    "        \n",
    "bi_freq = pd.DataFrame.from_dict(bi_frequency, orient = \"index\")\n",
    "bi_freq= bi_freq.sort_values(by=[0], ascending = False)\n",
    "Out[102]:\n",
    "0\n",
    "global warming\t380\n",
    "carbon tax\t219\n",
    "about climate\t93\n",
    "climate crisis\t60\n",
    "sea ice\t57\n",
    "...\t...\n",
    "sklippitt cosmic_engineer\t21\n",
    "trudeau liberals\t21\n",
    "polar bears\t21\n",
    "terrymorse stevecl58542482\t21\n",
    "change not\t21\n",
    "99 rows × 1 columns\n",
    "\n",
    "In [33]:\n",
    "# Count trigrams frequencies\n",
    "tri_frequency = defaultdict(int)\n",
    "for token in trigrams_list:\n",
    "        tri_frequency[token] += 1\n",
    "        \n",
    "        \n",
    "tri_freq = pd.DataFrame.from_dict(tri_frequency, orient = \"index\")\n",
    "tri_freq = tri_freq.sort_values(by=[0], ascending = False)\n",
    "\n",
    "\n",
    "# on remarque que c'est peu pertinent, on se concentre d'abord sur les mots et les bigrams\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-33-a4de6516e5f4> in <module>\n",
    "      1 # Count trigrams frequencies\n",
    "      2 tri_frequency = defaultdict(int)\n",
    "----> 3 for token in trigrams_list:\n",
    "      4         tri_frequency[token] += 1\n",
    "      5 \n",
    "\n",
    "NameError: name 'trigrams_list' is not defined\n",
    "In [32]:\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-32-0921cf8e9781> in <module>\n",
    "      2 \n",
    "      3 \n",
    "----> 4 frequency.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n",
    "\n",
    "AttributeError: 'collections.defaultdict' object has no attribute 'doc2idx'\n",
    "In [32]:\n",
    "# Count bigrams frequencies\n",
    "frequency = defaultdict(int)\n",
    "for token in bigrams_list:\n",
    "        frequency[token] += 1\n",
    "frequency\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-32-a97ed0b5eb1a> in <module>\n",
    "      1 # Count bigrams frequencies\n",
    "      2 frequency = defaultdict(int)\n",
    "----> 3 for token in bigrams_list:\n",
    "      4         frequency[token] += 1\n",
    "      5 frequency\n",
    "\n",
    "NameError: name 'bigrams_list' is not defined\n",
    "In [36]:\n",
    "\n",
    "Out[36]:\n",
    "''\n",
    "In [31]:\n",
    "dictionary = corpora.Dictionary(sents)\n",
    "Out[31]:\n",
    "<gensim.corpora.dictionary.Dictionary at 0x1b11236b280>\n",
    "In [29]:\n",
    "matrix = [dictionary.doc2bow(sent) for sent in sents]\n",
    "In [30]:\n",
    "\n",
    "Out[30]:\n",
    "[]\n",
    "In [ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
